{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Local LLM with Ollama\n",
    "\n",
    "Using local LLMs with Ollama offers several advantages over using externally hosted models:\n",
    "\n",
    "![Ollama](..//images//ollama.jpg)\n",
    "\n",
    "1. Enhanced Data Privacy and Security\n",
    "\n",
    "    On-Premise Control: By running the LLM on your local machine or private server, you retain complete control over your data. This is crucial when working with sensitive or proprietary information.\n",
    "    Reduced Risk of Data Leaks: Eliminates the need to send your data to third-party servers, minimizing the risk of unauthorized access or accidental exposure.\n",
    "\n",
    "2. Cost-Effectiveness\n",
    "\n",
    "    Free to Use: Ollama is an open-source project, meaning you can use it without incurring usage fees or subscription costs.\n",
    "    No Cloud Costs: Avoid the expenses associated with using cloud-based LLM services, especially for high-volume usage or larger models.\n",
    "\n",
    "3. Customization and Flexibility\n",
    "\n",
    "    Model Variety: Ollama supports various open-source models, giving you the freedom to choose the one that best suits your task.\n",
    "    Easy Model Switching: Easily switch between different models or experiment with different configurations without relying on external providers.\n",
    "    Fine-tuning: You have the ability to fine-tune models on your specific datasets to achieve better performance for your particular use case.\n",
    "\n",
    "4. Offline Access\n",
    "\n",
    "    Reliable Availability: Unlike cloud-based LLMs, which can experience downtime or connectivity issues, a local Ollama instance is always accessible, even without an internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we will create a adpative RAG agent system which will create reports for the Senior Executive Team from the SHAP Explnations of our model\n",
    "\n",
    "1.  We will use LangSmith To Track the chain we create\n",
    "2.  We will create the whole system in a graph agent Architecture using Langgraph\n",
    "3. This is inspired from the langchain explanation for adaptiv RAG - https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n",
    "\n",
    "LLM agents are developed using Llama3 - There are many other open source models available with ollama\n",
    "\n",
    "![Llama3](..//images//llama3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Github\\shap-llm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Work\\Github\\shap-llm\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\Work\\Github\\shap-llm\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.llms import Ollama\n",
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# nomic_embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#paraphrase-distilroberta-base-v2\n",
    "#all-distilroberta-v1\n",
    "#all-MiniLM-L6-v2\n",
    "#all-MiniLM-L12-v2\n",
    "\n",
    "###Using HuggingFace Embeddings over OpenAI Embeddings - Tried the one's above but this looks to give best result\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L12-v2'\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Objective here is to create a global report from whole data and not local summary. Hence we will not use shap_explanation file\n",
    "\n",
    "\n",
    "text = [\n",
    "    \"..//documents//data_dictionary.txt\",\n",
    "    \"..//documents//data_summary.txt\",\n",
    "    \"..//documents//shap_summary.txt\"\n",
    "]\n",
    "docs = [TextLoader(url).load() for url in text]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400, chunk_overlap=100\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"churn-rag-chroma-2\",\n",
    "    embedding=hf,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a Business Analyst and Data Science Manager to reports from the model. This is simple agents created by Prompt defention and simple RAG. \n",
    "\n",
    "Multiple questions are asked to the business analyst agent and then the answers for these questions are passed as context to Data Science Manger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def business_analyst1(question,model,top_k=7,fetch_k=20,retriver_type=\"normal\",search_type=\"similarity\"):\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" You are a business analyst for a telecom company. \n",
    "                Your job is to answer question from executives on Churn. \n",
    "                Answer ONLY with stats you got from the context provided.\n",
    "                DO NOT add any erronous stats.\n",
    "                Answer from context only.\n",
    "\n",
    "                Context: {context}\n",
    "\n",
    "                Question: {question}\"\"\",\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    if retriver_type==\"fetch_k\":\n",
    "        retriever1 = vectorstore.as_retriever(search_type=\"mmr\",\n",
    "                                          search_kwargs={'k': top_k, 'fetch_k': fetch_k})\n",
    "    elif retriver_type=='non-similar':\n",
    "        retriever1 = vectorstore.as_retriever(search_type=\"mmr\",\n",
    "                                          search_kwargs={'k': top_k, 'lambda_mult': 0.25})\n",
    "    else:\n",
    "        retriever1 = vectorstore.as_retriever(search_type='similarity',search_kwargs={'k': top_k})\n",
    "        \n",
    "    docs=retriever1.invoke(question)\n",
    "\n",
    "    print(len(docs))\n",
    "\n",
    "    #llm = Ollama(model=llm1)\n",
    "    llm0 = ChatOllama(model=model)\n",
    "    # Chain\n",
    "    rag_chain0 = prompt | llm0 | StrOutputParser()\n",
    "\n",
    "    # docs = retriever.invoke(question)\n",
    "    document=format_docs(docs)\n",
    "    answer = rag_chain0.invoke({\"question\": question,\"context\": document})\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "def manager(question,model,output1,output2,output3,output4):\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" You are manager of data science team for a telecom company. \n",
    "                You get different reports on churn from your business analyst in your team.\n",
    "                Your job is to format and proof read the report provided to you by the business analyst and create a enriched report for senior executive team within 1000 words.\n",
    "                The end user might not be tech savvy, so keep the language easy to understand\n",
    "                Answer should be properly formatted and user friendly and easy to understand.\n",
    "                class 1 is churn.\n",
    "                Use the below context for reference to make answer.\n",
    "\n",
    "                Context: {context}\n",
    "\n",
    "                Question: {question}\"\"\",\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    llm0 = ChatOllama(model=model)\n",
    "    # Chain\n",
    "    rag_chain0 = prompt | llm0 | StrOutputParser()\n",
    "\n",
    "    answer = rag_chain0.invoke({\"question\": question,\"context\": output1 + \"\\n\" + output2 + \"\\n\" + output3+\"\\n\"+output4})\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Based on the SHAP values, here are the top 20 features that increase the probability of class 1:\n",
      "\n",
      "1. contract - Month-to-month (increase: 16.49%)\n",
      "2. paymentmethod - Electronic check (increase: 4.13%)\n",
      "3. tenure - within (-0.001, 2.0] (increase: 13.86))\n",
      " SHAP Question: Look at all features and identify the top 20 features which increases the probability most.\n"
     ]
    }
   ],
   "source": [
    "output1=(business_analyst1(question=\"Look at all features and identify the top 20 features which increases the probability of class 1 the most. The highest increase in probability will be on top\",\n",
    "                        model=\"llama3:latest\",top_k=8,retriver_type='normal'))\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "After analyzing the feature importances, I found that the top 10 features that decrease the probability of class 1 the most are:\n",
      "\n",
      "1. **Two year contract**: Decreases probability by 19.73%\n",
      "2. **No internet service**: Decreases probability by 11.49% (for multiple lines)\n",
      "3. **No online security**: Decreases probability by 6.08%\n",
      "4. **No device protection**: Decreases probability by 5.05% (with tech support)\n",
      "5. **Two year tenure**: Decreases probability by 4.60% (payment method is mailed check)\n",
      "6. **No phone service**: Decreases probability by 4.50% (multiple lines and internet services)\n",
      "7. **One year contract**: Decreases probability by 3.55% (paperless billing)\n",
      "8. **No online backup**: Decreases probability by 3.35% (tech support is no)\n",
      "9. **DSL internet service**: Decreases probability by 2.88% (payment method is credit card automatic)\n",
      "10. **No streaming movies**: Decreases probability by 2.53% (tech support is yes)\n",
      "\n",
      "These features are likely to have a significant impact on the likelihood of a customer being classified as class 1.\n"
     ]
    }
   ],
   "source": [
    "output2=(business_analyst1(question=\"Look at all features and identify the top 10 features which decreases the probability of class 1 the most. The highest decrease in probability will be on top\",\n",
    "                        model=\"llama3:latest\",top_k=8,retriver_type='normal'))\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Based on the feature importances calculated using the `feature_importances_` attribute from the `RandomForestClassifier`, the top 10 features in terms of their importance in the model's predictions are:\n",
      "\n",
      "1. tenure (importance: 0.1452)\n",
      "2. totalcharges (importance: 0.1264)\n",
      "3. monthlycharges (importance: 0.1155)\n",
      "4. seniorcitizen (importance: 0.1048)\n",
      "5. onlinebackup (importance: 0.0956)\n",
      "6. deviceprotection (importance: 0.0869)\n",
      "7. techsupport (importance: 0.0791)\n",
      "8. streamingtv (importance: 0.0763)\n",
      "9. contract (importance: 0.0745)\n",
      "10. gender (importance: 0.0662)\n",
      "\n",
      "The lowest importance rank in the model prediction is on top, so the feature with the highest importance is actually `seniorcitizen`, which has an importance of 0.1048.\n",
      "\n",
      "Here's a rough interpretation of these results:\n",
      "\n",
      "- The most important features are related to customer tenure and billing information (tenure, totalcharges, monthlycharges), suggesting that longer-term customers who spend more on their plans are more likely to churn.\n",
      "- Seniorcitizen is the second most important feature, indicating that older customers may be more likely to switch providers due to changes in their personal or financial circumstances.\n",
      "- The remaining features are all related to customer preferences and services (onlinebackup, deviceprotection, techsupport, streamingtv), which suggests that customers who value these services are less likely to churn.\n"
     ]
    }
   ],
   "source": [
    "output3=(business_analyst1(question=\"Look at all features and identify the top 10 features importance rank in the model prediction. The lowest importance rank in the model prediction be on top\",\n",
    "                        model=\"llama3:latest\",top_k=8,retriver_type='normal'))\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "According to the context, the list of different action features are:\n",
      "\n",
      "1. PhoneService\n",
      "2. InternetService\n",
      "3. OnlineBackup\n",
      "4. OnlineSecurity\n",
      "5. DeviceProtection\n",
      "6. TechSupport\n",
      "7. Contract\n"
     ]
    }
   ],
   "source": [
    "output4=(business_analyst1(question=\"What are the list of different action features?\",\n",
    "                        model=\"llama3:latest\",top_k=3,retriver_type='normal'))\n",
    "print(output4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Churn Report**\n",
      "\n",
      "As a telecom company, understanding why customers leave us is crucial to improving our services and retaining valuable relationships. Based on the SHAP values and feature importances, I have compiled the top 20 features that increase or decrease the probability of class 1 (churn). Below are the key findings:\n",
      "\n",
      "**Top 20 Features that Increase Churn Probability:**\n",
      "\n",
      "The top 10 features that increase the probability of churn are:\n",
      "\n",
      "1. **Month-to-month contract**: Increases probability by 16.49%\n",
      "2. **Electronic check payment method**: Increases probability by 4.13%\n",
      "3. **Tenure within (-0.001, 2.0]**: Increases probability by 13.86%\n",
      "\n",
      "These features indicate that customers who are on month-to-month contracts and use electronic checks for payments are more likely to churn.\n",
      "\n",
      "**Top 20 Features that Decrease Churn Probability:**\n",
      "\n",
      "The top 10 features that decrease the probability of churn are:\n",
      "\n",
      "1. **Two-year contract**: Decreases probability by 19.73%\n",
      "2. **No internet service** (multiple lines): Decreases probability by 11.49%\n",
      "3. **Online security**: Decreases probability by 6.08%\n",
      "\n",
      "These features suggest that customers who have two-year contracts, use multiple lines without internet services, and opt for online security are less likely to churn.\n",
      "\n",
      "**Top Features in Model Predictions:**\n",
      "\n",
      "The top 10 features in terms of importance in the model's predictions are:\n",
      "\n",
      "1. **Tenure**: Importance: 0.14\n",
      "2. **Total charges**: Importance: 0.12\n",
      "3. **Monthly charges**: Importance: 0.11\n",
      "4. **Senior citizen**: Importance: 0.10\n",
      "5. **Online backup**: Importance: 0.09\n",
      "\n",
      "These features indicate that customer tenure, billing information, and senior citizen status are key factors in determining the likelihood of churn.\n",
      "\n",
      "**Recommendations to Improve Retention and Decrease Churn:**\n",
      "\n",
      "Based on our findings, I recommend the following top 5 actions to improve retention and decrease churn:\n",
      "\n",
      "1. **Offer flexible contract options**: Provide customers with the option to choose between month-to-month and longer-term contracts to cater to their needs.\n",
      "2. **Improve payment methods**: Offer alternative payment methods, such as credit card or bank transfers, to reduce the likelihood of electronic check payments being a contributing factor to churn.\n",
      "3. **Enhance internet services**: Provide reliable and high-speed internet services to customers with multiple lines to improve overall satisfaction and reduce churn.\n",
      "4. **Emphasize online security**: Highlight the importance of online security features to senior citizens and other customers who may be more susceptible to online threats.\n",
      "5. **Personalize services**: Offer personalized services, such as tailored plans and technical support, to customers based on their tenure, billing information, and preferences.\n",
      "\n",
      "By implementing these recommendations, we can improve customer satisfaction, reduce churn, and foster long-term relationships with our valued customers.\n"
     ]
    }
   ],
   "source": [
    "final_report=manager(question=f\"\"\"What are the top 5 reasons for churn?\\n\n",
    "                             What are top 5 recommended actions to improve retention and decrease churn?\"\"\",\n",
    "                             output1=output1,\n",
    "                             output2=output2,\n",
    "                             output3=output3,\n",
    "                             output4=output4,\n",
    "                             model=\"llama3:latest\")\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Report: Top 5 Recommended Actions to Improve Retention and Decrease Churn**\n",
      "\n",
      "**Executive Summary**\n",
      "\n",
      "Our data science team has analyzed the SHAP values and feature importances to identify the top 5 recommended actions to improve retention and decrease churn, specifically using action features. These findings are based on our analysis of customer behavior and preferences.\n",
      "\n",
      "**Recommended Actions**\n",
      "\n",
      "1. **Offer Phone Service**: Providing phone service to customers is crucial in reducing the likelihood of churn by 19.73%. This feature has a significant impact on customer satisfaction and loyalty.\n",
      "2. **Provide Internet Service**: Offering internet services, especially for multiple lines, can decrease the probability of churn by 11.49%. This highlights the importance of reliable connectivity in retaining customers.\n",
      "3. **Enhance Online Security**: Providing online security features to customers decreases the probability of churn by 6.08%. This emphasizes the need for robust digital protection measures to prevent data breaches and cyber attacks.\n",
      "4. **Offer Device Protection with Tech Support**: Offering device protection with tech support can decrease the probability of churn by 5.05% (with tech support). This suggests that customers value the convenience and security provided by these services, which can help reduce churn.\n",
      "5. **Promote Two-Year Contract**: While a two-year contract may not seem like an obvious retention strategy, our analysis shows it decreases the probability of churn by 4.60%. This could be due to the commitment and stability that comes with a longer-term contract.\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "* Providing phone service and internet services are critical in reducing churn\n",
      "* Enhancing online security features can significantly impact customer loyalty\n",
      "* Offering device protection with tech support can help reduce churn, especially for customers who value these services\n",
      "* Promoting two-year contracts may be an effective strategy to improve retention\n",
      "\n",
      "By implementing these recommended actions, our telecom company can proactively address the root causes of churn and improve overall customer satisfaction. By focusing on these key areas, we can create a more loyal customer base and drive long-term growth.\n",
      "\n",
      "**Recommendation**\n",
      "\n",
      "Based on our analysis, I recommend that the senior executive team prioritize the implementation of these top 5 recommended actions to improve retention and decrease churn. By leveraging action features such as phone service, internet services, online security, device protection with tech support, and two-year contracts, we can create a more loyal customer base and drive long-term growth for our company.\n",
      "\n",
      "**Word Count:** 996 words\n",
      "\n",
      "**References:**\n",
      "\n",
      "* SHAP values report\n",
      "* Feature importances report\n",
      "* Context provided by the business analyst\n",
      "\n",
      "I hope this enriched report meets your requirements. Let me know if you have any further questions or concerns!\n"
     ]
    }
   ],
   "source": [
    "response2=manager(question=f\"\"\"\n",
    "                             What are top 5 recommended actions to improve retention and decrease churn specifically using action features?\n",
    "                             Only use action features for this recommnedation\"\"\",\n",
    "                             output1=output1,\n",
    "                             output2=output2,\n",
    "                             output3=output3,\n",
    "                             output4=output4,\n",
    "                             model=\"llama3:latest\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### LangGraph Agents\n",
    "\n",
    "\n",
    "##### What are LangGraph Agents?\n",
    "\n",
    "    In the LangChain and LangGraph context, an agent is a sophisticated construct that utilizes language models (LLMs) to reason and decide on actions in an autonomous manner. Unlike traditional LangChain chains that follow a predefined sequence, agents dynamically determine their next steps based on the current situation and goals.\n",
    "\n",
    "##### Building Agents with LangGraph\n",
    "\n",
    "    Define Nodes: Each node in the LangGraph represents a function or a LangChain runnable (like an LLMChain).\n",
    "\n",
    "    Connect with Edges: Define edges to dictate the flow of information and control between nodes. You'll often have conditional edges where the next node is determined based on the output of the current node.\n",
    "    \n",
    "    Agent Logic: One or more nodes will typically contain the agent's decision-making logic, using an LLM to analyze the current state and choose the next action.\n",
    "\n",
    "\n",
    "We will try to implement an advanced RAG system for the above Task which will be more accurate. We will design this system as LangGraph agents. We will add the Business Analysts and Manager Agents into this architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangGraph Agent we will create will be having this architecture\n",
    "\n",
    "![Adaptive_RAG_Agent](..//images//Adaptive_RAG_Agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Retrieval Grader\n",
    "retrieval_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:latest\", format=\"json\", temperature=0)\n",
    "retrieval_grader = retrieval_prompt | llm | JsonOutputParser()\n",
    "retriever=vectorstore.as_retriever(search_type='similarity',search_kwargs={'k': 20})\n",
    "# docs = retriever.invoke(question)\n",
    "\n",
    "\n",
    "##Answer Generator\n",
    "buisiness_analyst_prompt = PromptTemplate(\n",
    "    template=\"\"\" You are a business analyst for a telecom company. \n",
    "            Your job is to create a report for senior executives on questions asked. \n",
    "            Answer ONLY with stats you got from the context provided.\n",
    "            DO NOT add any erronous stats.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "generator_llm = ChatOllama(model=\"llama3:latest\",temperature=0)\n",
    "# Chain\n",
    "business_analyst_rag_chain = buisiness_analyst_prompt | generator_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "##Manager Answer Generator\n",
    "manager_prompt = PromptTemplate(\n",
    "    template=\"\"\" You are manager of data science team for a telecom company. \n",
    "                You get different reports on churn from your business analyst in your team.\n",
    "                Your job is to format and proof read the report provided to you by the business analyst and create a enriched report for senior executive team.\n",
    "                The end user might not be tech savvy, so keep the language easy to understand\n",
    "                Answer should be properly formatted and user friendly and easy to understand.\n",
    "                class 1 is churn.\n",
    "                Use the below context for reference to make answer.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "manager_rag_chain = manager_prompt | generator_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "### Hallucination Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3:latest\", format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "hallucination_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "### Answer Grader \n",
    "\n",
    "\n",
    "# Prompt\n",
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question}\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n",
    "# answer_grader.invoke({\"question\": question,\"generation\": answer_generation})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question Re-writer\n",
    "rewrite_llm = ChatOllama(model=\"llama3:latest\", temperature=0)\n",
    "# Prompt \n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the initial and formulate an improved question. \\n\n",
    "     Here is the initial question: \\n\\n {question}.\n",
    "     ONLY RETURN THE REWRITTEN QUESTION.\\n\n",
    "     DONOT ADD ANY TEXT OTHER THAN REWRITTEN QUESTION.\\n\n",
    "     Improved question with no preamble: \\n \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | rewrite_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents \n",
    "        report: FInal report from Manager LLM\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : List[str]\n",
    "    report : str\n",
    "\n",
    "retriever=vectorstore.as_retriever(search_type='similarity',search_kwargs={'k': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def analyst_generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    context=format_docs(documents)\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = business_analyst_rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"analyst_generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": format_docs(documents), \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "def manager_generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    context=state[\"generation\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    report = manager_rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"question\": question, \"report\": report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Graph Agent\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"analyst_generate\", analyst_generate) # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query) # transform_query\n",
    "workflow.add_node(\"manager_generate\", manager_generate) # manager_query\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"analyst_generate\": \"analyst_generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyst_generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"analyst_generate\",\n",
    "        \"useful\": \"manager_generate\",\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"manager_generate\",END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'analyst_generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'manager_generate':\"\n",
      "'\\n---\\n'\n",
      "('**Churn Prediction Report**\\n'\n",
      " '\\n'\n",
      " '**Executive Summary**\\n'\n",
      " '\\n'\n",
      " 'Our analysis reveals that certain customer characteristics significantly '\n",
      " 'increase the likelihood of churn (Class 1). The top contributing factors '\n",
      " 'include:\\n'\n",
      " '\\n'\n",
      " '* **Month-to-month contract**: A month-to-month contract is associated with '\n",
      " 'a 16.49% higher probability of churn.\\n'\n",
      " '* **High tenure**: Tenure within the range (2.0, 5.0] and (-0.001, 2.0] both '\n",
      " 'contribute to a 7.47% and 13.86% increase in churn probability, '\n",
      " 'respectively.\\n'\n",
      " '* **Streaming services**: Customers with access to streaming TV and/or '\n",
      " 'movies are 1.55% to 1.80% more likely to churn.\\n'\n",
      " '* **Higher monthly charges**: Total charges within the range (59,390.18, '\n",
      " '87,672.45] lead to a 3.57% increase in churn probability.\\n'\n",
      " '\\n'\n",
      " 'These key factors indicate that customers with these characteristics are '\n",
      " 'more likely to belong to Class 1, highlighting areas for targeted retention '\n",
      " 'efforts.\\n'\n",
      " '\\n'\n",
      " '**Recommendations**\\n'\n",
      " '\\n'\n",
      " 'Based on our findings, we suggest:\\n'\n",
      " '\\n'\n",
      " '* Offering competitive pricing and contract options to customers with '\n",
      " 'month-to-month contracts.\\n'\n",
      " '* Developing personalized retention strategies for high-tenure customers.\\n'\n",
      " '* Enhancing streaming services and promotions to retain customers who value '\n",
      " 'these offerings.\\n'\n",
      " '* Reviewing pricing structures for customers with higher monthly charges.\\n'\n",
      " '\\n'\n",
      " 'By addressing these key factors, we can proactively reduce churn and improve '\n",
      " 'customer satisfaction.\\n'\n",
      " '\\n'\n",
      " '**Conclusion**\\n'\n",
      " '\\n'\n",
      " 'Our analysis provides valuable insights into the top contributors to churn. '\n",
      " 'By understanding these factors, we can develop targeted strategies to '\n",
      " 'mitigate churn and drive business growth.\\n'\n",
      " '\\n'\n",
      " '**References**\\n'\n",
      " '\\n'\n",
      " '* SHAP values and feature importance analysis\\n'\n",
      " '\\n'\n",
      " 'Note: The report is formatted with clear headings, concise language, and '\n",
      " 'easy-to-understand explanations. The recommendations are actionable and '\n",
      " 'focused on specific areas for improvement.')\n"
     ]
    }
   ],
   "source": [
    "##Test the Agent with a Question\n",
    "\n",
    "# Run \n",
    "inputs = {\"question\": \"What are the top reasons which increases the probability of class 1?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'analyst_generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'manager_generate':\"\n",
      "'\\n---\\n'\n",
      "('**Churn Reduction Report**\\n'\n",
      " '\\n'\n",
      " '**Executive Summary**\\n'\n",
      " '\\n'\n",
      " 'Our analysis reveals five key opportunities to increase customer retention '\n",
      " 'and reduce churn. By targeting specific customer segments with tailored '\n",
      " 'offers, we can capitalize on untapped potential and drive business growth.\\n'\n",
      " '\\n'\n",
      " '**Recommended Actions**\\n'\n",
      " '\\n'\n",
      " '1. **Pitch Phone Service**: Focus on customers without phone service who '\n",
      " 'have a partner (35%) or dependents (27%). Offering phone services to these '\n",
      " 'customers could lead to an increase in retention.\\n'\n",
      " '2. **Upgrade Internet Service**: Target customers with online security but '\n",
      " 'no internet service, particularly those with multiple lines (42%). Pitching '\n",
      " 'DSL or Fiber optic internet services to these customers could reduce churn.\\n'\n",
      " '3. **Offer Online Backup**: Focus on customers without online backup who '\n",
      " 'have dependents (25%) or a partner (20%). Offering online backup services to '\n",
      " 'these customers could increase retention.\\n'\n",
      " '4. **Promote Device Protection**: Target customers with device protection '\n",
      " 'but no internet service, particularly those with multiple lines (30%). '\n",
      " 'Pitching device protection services to these customers could reduce churn.\\n'\n",
      " '5. **Convert Month-to-Month Contracts**: Focus on senior citizens (40%) and '\n",
      " 'customers with dependents (35%) who are currently on month-to-month '\n",
      " 'contracts. Offering two-year contracts to these customers could increase '\n",
      " 'retention.\\n'\n",
      " '\\n'\n",
      " '**Key Insights**\\n'\n",
      " '\\n'\n",
      " '* 35% of customers without phone service have a partner, highlighting an '\n",
      " 'opportunity to pitch phone services.\\n'\n",
      " '* 42% of customers with online security but no internet service have '\n",
      " 'multiple lines, indicating potential for upselling internet services.\\n'\n",
      " '* 25% of customers without online backup have dependents, and 20% have a '\n",
      " 'partner, suggesting opportunities to offer online backup services.\\n'\n",
      " '* 30% of customers with device protection but no internet service have '\n",
      " 'multiple lines, providing an opportunity to promote device protection '\n",
      " 'services.\\n'\n",
      " '* 40% of customers on month-to-month contracts are senior citizens, and 35% '\n",
      " 'have dependents, indicating potential for converting these customers to '\n",
      " 'two-year contracts.\\n'\n",
      " '\\n'\n",
      " '**Next Steps**\\n'\n",
      " '\\n'\n",
      " 'Implement these recommendations to increase customer retention and reduce '\n",
      " 'churn. Monitor the effectiveness of each action and adjust strategies as '\n",
      " 'needed to optimize results.\\n'\n",
      " '\\n'\n",
      " '**Conclusion**\\n'\n",
      " '\\n'\n",
      " 'By targeting specific customer segments with tailored offers, we can drive '\n",
      " 'business growth and improve overall customer satisfaction. These five '\n",
      " 'recommended actions provide a solid foundation for reducing churn and '\n",
      " 'increasing retention.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"What are 5 recommended actions to increase retention or to reduce churn?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coudn't visalize graph directly within notebook. There might be a way but I printed the mermaid graph structure and the used this to plot graph on a online mermaid live service\n",
    "\n",
    "https://mermaid.live/edit#pako:eNqNVF1rGzEQ_CtCxdiFOK-hFyiYmKQvTalb-uILZi2tbIFOMntSQjD336u76L7wGfK2Ws3MSrMrnblwEnnGZ7Ozttpn7DxXxr2JI5Cf1ysR6BVjNDfaItC8qqrZLLcHgtOR_V3f5zb3u13pI3632_4OWHrt7EuWZU1OGCjLBEIrI-RRWzBsgydHvobFbA_aoCeNr0iLbRe-fI0o549IPW7tRCjQ-icCWYPH6ynGKlZ9L_1imwK2OkT8FHKDiqDA9ir1SZoE6y43QfoJFg71SVJwXf4JLRJ4_EVJd7FtU2xly7fIdcQ-UzPR4m5rQ59h153ousWWy--ss3l4jWYnNWzcl2Zn7PaUaRPSY1IDuLBi2iC2vL3wg902BUZFr7Pbnn9MXcNNqeFsDM_UmTrp9FAz9UyX7AcYE0Qcb4_yosaUxLMbcOpLPTgiFJ3mI7miq1MLpvZ8XnAffJ38B0bLKdVrLjbTskbF-kfMlDYm-6KUVHsYg9onnCB7UEp8G0P6GWx1QN5JvOc3vEAqQMv4CZ1zy1jOI7TAnGcxlKggGJ_z3FYRCsG7P-9W8MxTwBseTjKavdYQv6PiI1n9B-nrv8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__[__start__]:::startclass;\n",
      "\t__end__[__end__]:::endclass;\n",
      "\tretrieve([retrieve]):::otherclass;\n",
      "\tgrade_documents([grade_documents]):::otherclass;\n",
      "\tanalyst_generate([analyst_generate]):::otherclass;\n",
      "\ttransform_query([transform_query]):::otherclass;\n",
      "\tmanager_generate([manager_generate]):::otherclass;\n",
      "\t__start__ --> retrieve;\n",
      "\tmanager_generate --> __end__;\n",
      "\tretrieve --> grade_documents;\n",
      "\ttransform_query --> retrieve;\n",
      "\tgrade_documents -.-> transform_query;\n",
      "\tgrade_documents -.-> analyst_generate;\n",
      "\tanalyst_generate -. not supported .-> analyst_generate;\n",
      "\tanalyst_generate -. useful .-> manager_generate;\n",
      "\tanalyst_generate -. not useful .-> transform_query;\n",
      "\tclassDef startclass fill:#ffdfba;\n",
      "\tclassDef endclass fill:#baffc9;\n",
      "\tclassDef otherclass fill:#fad7de;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
